package org.talend.logging.audit.kafka;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.slf4j.MDC;
import org.talend.logging.audit.LogLevel;
import org.talend.logging.audit.impl.AbstractBackend;
import org.talend.logging.audit.impl.AuditConfiguration;
import org.talend.logging.audit.impl.AuditConfigurationMap;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;

public class KafkaBackend extends AbstractBackend {

    private final KafkaProducer<String, String> kafkaProducer;

    private final String kafkaTopic;

    private final String partitionKeyName;

    private final String bootstrapServers;

    private final Long blockTimeoutMs;

    private final ObjectMapper objectMapper = new ObjectMapper();

    public KafkaBackend(AuditConfigurationMap config) {
        super(null);
        StringSerializer keyValueSerializer = new StringSerializer();
        this.bootstrapServers = config.getString(AuditConfiguration.KAFKA_BOOTSTRAP_SERVERS);
        this.kafkaTopic = config.getString(AuditConfiguration.KAFKA_TOPIC);
        this.partitionKeyName = config.getString(AuditConfiguration.KAFKA_PARTITION_KEY_NAME);
        this.blockTimeoutMs = config.getLong(AuditConfiguration.KAFKA_BLOCK_TIMEOUT_MS);
        Map<String, Object> producerConfig = new HashMap<>();
        producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        producerConfig.put(ProducerConfig.ACKS_CONFIG, "1");
        producerConfig.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, blockTimeoutMs);
        this.kafkaProducer = new KafkaProducer<>(producerConfig, keyValueSerializer, keyValueSerializer);
    }

    public KafkaBackend(KafkaProducer<String, String> kafkaProducer, String kafkaTopic, String partitionKeyName,
            String bootstrapServers, Long blockTimeoutMs) {
        super(null);
        this.kafkaProducer = kafkaProducer;
        this.kafkaTopic = kafkaTopic;
        this.partitionKeyName = partitionKeyName;
        this.bootstrapServers = bootstrapServers;
        this.blockTimeoutMs = blockTimeoutMs;
    }

    @Override
    public void log(String category, LogLevel level, String message, Throwable throwable) {
        try {
            this.kafkaProducer.send(createRecordFromContext(getCopyOfContextMap())).get(this.blockTimeoutMs,
                    TimeUnit.MILLISECONDS);
        } catch (InterruptedException | ExecutionException | TimeoutException e) {
            throw new RuntimeException("Failure when sending the audit log to Kafka", e);
        }
    }

    private ProducerRecord<String, String> createRecordFromContext(Map<String, String> context) {
        String key = context != null ? context.getOrDefault(this.partitionKeyName, null) : null;
        String value;
        try {
            value = this.objectMapper.writeValueAsString(context);
            return new ProducerRecord<>(this.kafkaTopic, null, System.currentTimeMillis(), key, value);
        } catch (JsonProcessingException e) {
            throw new RuntimeException("Failure while mapping the audit log to JSON", e);
        }
    }

    @Override
    public Map<String, String> getCopyOfContextMap() {
        return MDC.getCopyOfContextMap();
    }

    @Override
    public void setContextMap(Map<String, String> newContext) {
        MDC.setContextMap(newContext);
    }

    String getKafkaTopic() {
        return kafkaTopic;
    }

    String getPartitionKeyName() {
        return partitionKeyName;
    }

    String getBootstrapServers() {
        return bootstrapServers;
    }

    Long getBlockTimeoutMs() {
        return blockTimeoutMs;
    }
}
